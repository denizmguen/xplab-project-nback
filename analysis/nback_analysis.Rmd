---
title: "NBACK AND LURES - Statistical Analysis"
date: 'Thursday,  August 1 08:00 CET'
author: "Deniz Guen"
output: html_document
---

# Required R packages
* `tidyverse` (or `ggplot2`, `dplyr`, `purrr`, `tibble`)
* `brms`

### Importing Libraries

```{r setup, include=FALSE, echo = FALSE, message = FALSE}
library(tidyverse)
library(rstan)
library(brms)
library(lme4)
options(mc.cores = max(parallel::detectCores()-1, 1)) # at least one core if the machine only has one
rstan_options(auto_write = TRUE)
```

### Setting seed for pseudo random operations
```{r setting seed}
set.seed(100) # this ensures that any psuedo-random computations are always the same
```



# Description
This file is a documentation of the statistical procedures that were used to analyse the data from the nback experiment that was deployed for the report 'The Importance of controlling for lure foils in nback tasks' by Deniz M. GÃ¼n


### Background: nback task
In the nback task participants are shown a sequence of stimuli. For every stimulus the participants have to indicate wether the same stimulus has been presented exactly n steps earlier. There are different variants that may require participants to pay attention to more than one modality at the same time (e.g. shape and color or number and sound). But for this experiment a simple variant was chosen.

The participants absolved 3- and 4back tasks with 100 trials for each, where the stimuli were integers between 1 and 9. They had 4 seconds to respond for each stimulus. 

There were two groups that differed in their porbability for lure foils. In the control group the lure foils were just as likely as any other number (1/9) while in the main groups the lure foils were assigned with a probability of 1/4 each. The control group is supposed to demonstrate that the lower accuracy for lure foils is not a result of some unknown effect that simply arises from having more lure foils.


### Purpose
This experiment is supposed to demonstrate that the accuracy is significantly worse for lure foils than for non-targets and thus that the overall accuracy may depend on the amount of lure foils and targets present in the data. Therefore, when deploying the nback task, the frequency of lure foils is a parameter that needs to be controlled in order to make sure that interpersonal differences are truly a result of ability and not the frequency of lure foils. 



# 1. Overview

#### Importing data
```{r Import Data,message=TRUE, warning=TRUE, tidy=FALSE}
data = data.frame(read.csv("fake_data.csv"))
head(data)
```

# 2. Data Cleaning



#### Removing non-pertinent data (practice sessions)
This includes every trial number smaller than n. Taking the 4back task for example, a target can not appear before the 4th trial.

```{r DataClean1}
data_pertinent = subset(data, trial_name != "practice" & trial_number > n)
```






#### Removing irrelevant columns
The only relevant columns for now are the stimulus type, the correctness, the reaction time, trial name, n-condition, the trial number and the submission id (subject)

```{r RelevantColumns}
data = subset(data_pertinent, select= c(stim_type,correctness,RT,trial_name,n,trial_number,submission_id))
```






#### Removing obviously Faulty Trials 
Subjects with less than 200 trials overall:
```{r Grouping}
grouped = count(group_by(data, submission_id))
ids_faulty = filter(grouped, n < 200)$submission_id
data = subset(data, !(submission_id %in% ids_faulty))
```





#### Removing outliers with respect to Reaction Times (applying Rosner Test)
Should there be outliers, all entries for a specific submission ID will be removed
```{r Outlier Detection Function}

normalize = function(data) {
  mu = mean(data)
  sig = sd(data)
  normalized = abs(data - mu)/sig
  return(normalized)
}

rosner_test = function(data, var.col, crit.value){

  data$normalized = normalize(var.col)
  maximum = c( max(data$normalized), min(data$normalized) ) %>% abs() %>% max()
  
  if(maximum >= crit.value){
    data <- subset(data, abs(data$normalized) != maximum)
    return(rosner_test(data, data$normalized, crit.value))
    
  } else {
    return(data)
  }
}
```

Getting Means for reaction times
```{r}
mean_rts = aggregate(data$RT, FUN=mean, by=list(data$submission_id, data$n))
names(mean_rts) <- c("submission_id", "n", "x")

mean_rts.woo = rosner_test(data=mean_rts, var.col=mean_rts$x, crit.value=3)

mean_rts.woo <- select(mean_rts.woo, c(1:(length(names(mean_rts.woo)))))
mean_rts.woo

mean_rts
```




#### Adding column stim_type_gross

```{r Add gross stimtype}
gross = data$stim_type
gross = ifelse(data$stim_type %in% c("non-target", "target"), "non-lure", "lure")
data$gross_stimtype = gross
```





#### Creating a table with accuracy for different IDs and stimuli

###### The grouping is important for counting the stimuli for a specific combination of values (e.g.: ID = 1, stim_type = "non-target", correctness = "correct", n=3, etc..)  such that the acuracy for a specific kind of stimulus can be calculated.

```{r count group members}
#Grouping by ID stim type, trial name and n --- 'nn' is the total number of trials for the specific variable combination of each row
grouped1 = data %>% group_by(submission_id,stim_type,gross_stimtype,trial_name,n)
cg1 = count(grouped1)
#Grouping by correctness for counts of "correct" or "incorrect" --- 'nn' is number of correct trials for the specific variable combination of each row
grouped2 = grouped1 %>% group_by(correctness, add=TRUE)
cg2 = count(grouped2)
# Getting list with unique values to iterate through in the following cells
IDs = unique(grouped1$submission_id)
stim_types = unique(grouped1$stim_type)
gross_stimtypes = unique(grouped1$gross_stimtype)
trial_names = c("main", "control")
N = c("3", "4")
```



##### Creating an accuracy table template

```{r Accuracy Table template}
# Accuracy table must be a table with the following rows: ID, trial_name, n, stimulus_type, gross_stimulus_type and accuracy
accuracy_table <- data.frame(matrix(ncol = 6, nrow = 0))
colnames(accuracy_table) <- c("ID","trial_name", "stimulus_type","gross_stimulus_type","n", "accuracy")

# Defining the right variable types for cols.
accuracy_table$submission_id <- as.factor(accuracy_table$submission_id)
accuracy_table$accuracy <- as.numeric(accuracy_table$accuracy)
accuracy_table$n <- as.factor(accuracy_table$n)

```



#### Filling Accuracy Tables

###### Fill accu_id
This table is later used to show participants who wish to know theirresults how they scored in comparison to other participants. 


```{r Fill Calculating accuracies by test subjects}

accu_id = subset(accuracy_table, select=c(ID,trial_name,n,accuracy)) # seperate n's

for(id in IDs){
  for(n in N){
    howmanyback = n
    trial_name = filter(data,submission_id==id)$trial_name[1]
    n_correct = filter(cg2, (submission_id == id) && (n == howmanyback & correctness == "correct"))$nn
    n_correct = sum(n_correct)
    n_total = 100
    accuracy = n_correct/n_total


    entry = data.frame(submission_id=id, trial_name=trial_name, n=n, accuracy=accuracy)
    accu_id <- rbind(accu_id, entry)
  }
}

head(accu_id)
```




###### Fill a complete accuracy table ordered on all relevant variables.

```{r Fill a complete accu table}
accu_complete <- subset(accuracy_table, select=c(ID, trial_name, stimulus_type, gross_stimulus_type, n, accuracy))

for(id in IDs){
  for(n in N){
    for(st in stim_types){
    
      howmanyback = n
      trial_name = filter(data,submission_id==id)$trial_name[1]
      gr_st = filter(data, stim_type == st)$gross_stimtype[1]
      n_correct = sum( filter(cg2, (submission_id == id && stim_type == st) && (n == howmanyback && correctness == "correct"))$nn )
      n_total = sum( filter(cg2, (submission_id == id && n == howmanyback) && stim_type == st)$nn )
      accuracy = n_correct/n_total
      
      entry = data.frame(submission_id = id, trial_name = trial_name, stimulus_type = st, gross_stimulus_type = gr_st, n = howmanyback, accuracy = accuracy, count = n_total)
      accu_complete <- rbind(accu_complete, entry)
    }
  }
}
head(accu_complete)
```


##### Variables
Dependent: Accuracy
Independent: trial_name, (gross_)stimulus_type, id, n, count

```{r Setting Variable Types}
accu_complete$submission_id <- as.factor(accu_complete$submission_id)
accu_complete$n <- as.factor(accu_complete$n)
```




# 3. Hypothesis Tests

#### The proposed hypotheses were


H1: The accuracy for lure foils (at n-1 as well as at n+1) is lower than the accuracy of non-targets. 

H2: The accuracy of targets is worse than the accuracy of non-targets




###### Getting an Overview

First we get an overview to illustrate the data
```{r Boxplot Accuracy}
ggplot(accu_complete,mapping=aes(x=stimulus_type, y=accuracy, color=n))+
  geom_boxplot()
```




### Removing Outliers with respect to accuracy and plotting again to quickly see wether outliers had an obvious impact.

```{r Removing Outliers and plotting again}
accu_complete <- rosner_test(accu_complete, var.col=accu_complete$accuracy, crit.value=3)
ggplot(data=accu_complete, mapping=aes(x=stimulus_type, y=accuracy, color=n))+
  geom_boxplot()
```


### Model
```{r Listing the Variables}
names(accu_complete)
```

We'll make accuracy in our formula dependend on the fixed effects stimulus_type and n and include the random intercepts for subjects (="submission_id") and groups (="trial_name")

```{r Regression Formula}
formula = accuracy ~ stimulus_type+n + (1|submission_id) + (1|trial_name)
```





Now we create a bayesian regression model 
```{r Creating brms model }

accu_complete.mdl.brm = brm(formula, accu_complete)
accu_complete.mdl.brm

```


# Testing Hypotheses

In order to test our hypotheses we look at the coefficients from our posterior samples, which we obtained from the model above. 

If a coefficient, say e.g. "b_stimulus_typetarget" of one sample is smaller than 0, this indicates the following:
Based on our estimates for the real parameters, there is a sample drawn from our posterior distribution for which it is true, that the mean accuracy for the stimulus type "target" is lower than the mean accuracy for the stimulus type "non-target" which is our intercept. 

What we are going to do in order to assess the probabilities of our hypotheses, is, look at the ratio of samples for which a coefficient like e.g. "b_stimulus_typetarget" is smaller than 0.


We predict that the accuracy is lower for all kinds of lures than it is for non-targets. We'll accept in a preliminary fashion that our hypotheses are true if the probability that one of the mentioned coefficients is lower than the intercept is at least 95%. 

```{r Calculating probabilities of Hypotheses}
post.samples = posterior_samples(accu_complete.mdl.brm)
probabilities = lapply(post.samples[, 1:8], FUN=function(x){return(mean(x<0))})

print(probabilities)
```

For our fake data we can actually see very high probabilites for our proposed hypotheses. Now the question remains wether our real data will yield the same results. 


